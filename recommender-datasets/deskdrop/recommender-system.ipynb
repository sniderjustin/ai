{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-05T14:01:45.721047Z","iopub.execute_input":"2022-05-05T14:01:45.721323Z","iopub.status.idle":"2022-05-05T14:01:45.742566Z","shell.execute_reply.started":"2022-05-05T14:01:45.721294Z","shell.execute_reply":"2022-05-05T14:01:45.741593Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Recommender System\n\nWritten using code from Gabriel Moreira's example notebook [here](https://www.kaggle.com/code/gspmoreira/recommender-systems-in-python-101/notebook).\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport scipy\nimport pandas as pd\nimport math\nimport random\nimport sklearn\nfrom nltk.corpus import stopwords\nfrom scipy.sparse import csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse.linalg import svds\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:45.771550Z","iopub.execute_input":"2022-05-05T14:01:45.772498Z","iopub.status.idle":"2022-05-05T14:01:47.587676Z","shell.execute_reply.started":"2022-05-05T14:01:45.772460Z","shell.execute_reply":"2022-05-05T14:01:47.586940Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"articles_df = pd.read_csv('/kaggle/input/articles-sharing-reading-from-cit-deskdrop/shared_articles.csv')\narticles_df = articles_df[articles_df['eventType'] == 'CONTENT SHARED']\narticles_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:47.589406Z","iopub.execute_input":"2022-05-05T14:01:47.590116Z","iopub.status.idle":"2022-05-05T14:01:48.104125Z","shell.execute_reply.started":"2022-05-05T14:01:47.590075Z","shell.execute_reply":"2022-05-05T14:01:48.103226Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"interactions_df = pd.read_csv('/kaggle/input/articles-sharing-reading-from-cit-deskdrop/users_interactions.csv')\ninteractions_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:48.105436Z","iopub.execute_input":"2022-05-05T14:01:48.105720Z","iopub.status.idle":"2022-05-05T14:01:48.472454Z","shell.execute_reply.started":"2022-05-05T14:01:48.105687Z","shell.execute_reply":"2022-05-05T14:01:48.471447Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Assign value for different event types\nevent_type_strength = {\n   'VIEW': 1.0,\n   'LIKE': 2.0, \n   'BOOKMARK': 2.5, \n   'FOLLOW': 3.0,\n   'COMMENT CREATED': 4.0,  \n}\n\ninteractions_df['eventStrength'] = interactions_df['eventType'].apply(lambda x: event_type_strength[x])","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:48.474876Z","iopub.execute_input":"2022-05-05T14:01:48.475453Z","iopub.status.idle":"2022-05-05T14:01:48.509478Z","shell.execute_reply.started":"2022-05-05T14:01:48.475377Z","shell.execute_reply":"2022-05-05T14:01:48.508635Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Keep only users with at least 5 interactions to avoid cold start problem\nusers_interactions_count_df = interactions_df.groupby(['personId', 'contentId']).size().groupby('personId').size()\nprint('# users: %d' % len(users_interactions_count_df))\nusers_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df >= 5].reset_index()[['personId']]\nprint('# users with at least 5 interactions: %d' % len(users_with_enough_interactions_df))","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:48.511009Z","iopub.execute_input":"2022-05-05T14:01:48.511342Z","iopub.status.idle":"2022-05-05T14:01:48.553272Z","shell.execute_reply.started":"2022-05-05T14:01:48.511299Z","shell.execute_reply":"2022-05-05T14:01:48.552230Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print('# of interactions: %d' % len(interactions_df))\ninteractions_from_selected_users_df = interactions_df.merge(users_with_enough_interactions_df, \n               how = 'right',\n               left_on = 'personId',\n               right_on = 'personId')\nprint('# of interactions from users with at least 5 interactions: %d' % len(interactions_from_selected_users_df))","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:48.554767Z","iopub.execute_input":"2022-05-05T14:01:48.555839Z","iopub.status.idle":"2022-05-05T14:01:48.596727Z","shell.execute_reply.started":"2022-05-05T14:01:48.555795Z","shell.execute_reply":"2022-05-05T14:01:48.595677Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Group person and content pairs. Provide event grouped event score\ndef smooth_user_preference(x):\n    return math.log(1+x, 2)\n    \ninteractions_full_df = interactions_from_selected_users_df \\\n                    .groupby(['personId', 'contentId'])['eventStrength'].sum() \\\n                    .apply(smooth_user_preference).reset_index()\nprint('# of unique user/item interactions: %d' % len(interactions_full_df))\ninteractions_full_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:48.598064Z","iopub.execute_input":"2022-05-05T14:01:48.598292Z","iopub.status.idle":"2022-05-05T14:01:48.665804Z","shell.execute_reply.started":"2022-05-05T14:01:48.598265Z","shell.execute_reply":"2022-05-05T14:01:48.664754Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Create train and test datasets\ninteractions_train_df, interactions_test_df = train_test_split(interactions_full_df,\n                                   stratify=interactions_full_df['personId'], \n                                   test_size=0.20,\n                                   random_state=42)\n\nprint('# interactions on Train set: %d' % len(interactions_train_df))\nprint('# interactions on Test set: %d' % len(interactions_test_df))","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:48.667317Z","iopub.execute_input":"2022-05-05T14:01:48.667560Z","iopub.status.idle":"2022-05-05T14:01:48.719948Z","shell.execute_reply.started":"2022-05-05T14:01:48.667528Z","shell.execute_reply":"2022-05-05T14:01:48.718830Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Indexing by personId to speed up the searches during evaluation\ninteractions_full_indexed_df = interactions_full_df.set_index('personId')\ninteractions_train_indexed_df = interactions_train_df.set_index('personId')\ninteractions_test_indexed_df = interactions_test_df.set_index('personId')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:48.721418Z","iopub.execute_input":"2022-05-05T14:01:48.721795Z","iopub.status.idle":"2022-05-05T14:01:48.732361Z","shell.execute_reply.started":"2022-05-05T14:01:48.721746Z","shell.execute_reply":"2022-05-05T14:01:48.731291Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def get_items_interacted(person_id, interactions_df):\n    # Get the user's data and merge in the movie information.\n    interacted_items = interactions_df.loc[person_id]['contentId']\n    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:48.735895Z","iopub.execute_input":"2022-05-05T14:01:48.736276Z","iopub.status.idle":"2022-05-05T14:01:48.745455Z","shell.execute_reply.started":"2022-05-05T14:01:48.736228Z","shell.execute_reply":"2022-05-05T14:01:48.744495Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Top-N accuracy metrics consts\nEVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS = 100\n\nclass ModelEvaluator:\n\n\n    def get_not_interacted_items_sample(self, person_id, sample_size, seed=42):\n        interacted_items = get_items_interacted(person_id, interactions_full_indexed_df)\n        all_items = set(articles_df['contentId'])\n        non_interacted_items = all_items - interacted_items\n\n        random.seed(seed)\n        non_interacted_items_sample = random.sample(non_interacted_items, sample_size)\n        return set(non_interacted_items_sample)\n\n    def _verify_hit_top_n(self, item_id, recommended_items, topn):        \n            try:\n                index = next(i for i, c in enumerate(recommended_items) if c == item_id)\n            except:\n                index = -1\n            hit = int(index in range(0, topn))\n            return hit, index\n\n    def evaluate_model_for_user(self, model, person_id):\n        #Getting the items in test set\n        interacted_values_testset = interactions_test_indexed_df.loc[person_id]\n        if type(interacted_values_testset['contentId']) == pd.Series:\n            person_interacted_items_testset = set(interacted_values_testset['contentId'])\n        else:\n            person_interacted_items_testset = set([int(interacted_values_testset['contentId'])])  \n        interacted_items_count_testset = len(person_interacted_items_testset) \n\n        #Getting a ranked recommendation list from a model for a given user\n        person_recs_df = model.recommend_items(person_id, \n                                               items_to_ignore=get_items_interacted(person_id, \n                                                                                    interactions_train_indexed_df), \n                                               topn=10000000000)\n\n        hits_at_5_count = 0\n        hits_at_10_count = 0\n        #For each item the user has interacted in test set\n        for item_id in person_interacted_items_testset:\n            #Getting a random sample (100) items the user has not interacted \n            #(to represent items that are assumed to be no relevant to the user)\n            non_interacted_items_sample = self.get_not_interacted_items_sample(person_id, \n                                                                          sample_size=EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS, \n                                                                          seed=item_id%(2**32))\n\n            #Combining the current interacted item with the 100 random items\n            items_to_filter_recs = non_interacted_items_sample.union(set([item_id]))\n\n            #Filtering only recommendations that are either the interacted item or from a random sample of 100 non-interacted items\n            valid_recs_df = person_recs_df[person_recs_df['contentId'].isin(items_to_filter_recs)]                    \n            valid_recs = valid_recs_df['contentId'].values\n            #Verifying if the current interacted item is among the Top-N recommended items\n            hit_at_5, index_at_5 = self._verify_hit_top_n(item_id, valid_recs, 5)\n            hits_at_5_count += hit_at_5\n            hit_at_10, index_at_10 = self._verify_hit_top_n(item_id, valid_recs, 10)\n            hits_at_10_count += hit_at_10\n\n        #Recall is the rate of the interacted items that are ranked among the Top-N recommended items, \n        #when mixed with a set of non-relevant items\n        recall_at_5 = hits_at_5_count / float(interacted_items_count_testset)\n        recall_at_10 = hits_at_10_count / float(interacted_items_count_testset)\n\n        person_metrics = {'hits@5_count':hits_at_5_count, \n                          'hits@10_count':hits_at_10_count, \n                          'interacted_count': interacted_items_count_testset,\n                          'recall@5': recall_at_5,\n                          'recall@10': recall_at_10}\n        return person_metrics\n\n    def evaluate_model(self, model):\n        #print('Running evaluation for users')\n        people_metrics = []\n        for idx, person_id in enumerate(list(interactions_test_indexed_df.index.unique().values)):\n            #if idx % 100 == 0 and idx > 0:\n            #    print('%d users processed' % idx)\n            person_metrics = self.evaluate_model_for_user(model, person_id)  \n            person_metrics['_person_id'] = person_id\n            people_metrics.append(person_metrics)\n        print('%d users processed' % idx)\n\n        detailed_results_df = pd.DataFrame(people_metrics) \\\n                            .sort_values('interacted_count', ascending=False)\n        \n        global_recall_at_5 = detailed_results_df['hits@5_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n        global_recall_at_10 = detailed_results_df['hits@10_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n        \n        global_metrics = {'modelName': model.get_model_name(),\n                          'recall@5': global_recall_at_5,\n                          'recall@10': global_recall_at_10}    \n        return global_metrics, detailed_results_df\n    \nmodel_evaluator = ModelEvaluator()    ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:48.747821Z","iopub.execute_input":"2022-05-05T14:01:48.748449Z","iopub.status.idle":"2022-05-05T14:01:48.774957Z","shell.execute_reply.started":"2022-05-05T14:01:48.748397Z","shell.execute_reply":"2022-05-05T14:01:48.773755Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Computes the most popular items\nitem_popularity_df = interactions_full_df.groupby('contentId')['eventStrength'].sum().sort_values(ascending=False).reset_index()\nitem_popularity_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:48.776754Z","iopub.execute_input":"2022-05-05T14:01:48.777454Z","iopub.status.idle":"2022-05-05T14:01:48.808573Z","shell.execute_reply.started":"2022-05-05T14:01:48.777402Z","shell.execute_reply":"2022-05-05T14:01:48.807698Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Create Popularity Model\n\nFind and reccomend the most popular items. \n\nDoes not customize reccomendations to the user. ","metadata":{}},{"cell_type":"code","source":"class PopularityRecommender:\n    \n    MODEL_NAME = 'Popularity'\n    \n    def __init__(self, popularity_df, items_df=None):\n        self.popularity_df = popularity_df\n        self.items_df = items_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n        \n    def recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):\n        # Recommend the more popular items that the user hasn't seen yet.\n        recommendations_df = self.popularity_df[~self.popularity_df['contentId'].isin(items_to_ignore)] \\\n                               .sort_values('eventStrength', ascending = False) \\\n                               .head(topn)\n\n        if verbose:\n            if self.items_df is None:\n                raise Exception('\"items_df\" is required in verbose mode')\n\n            recommendations_df = recommendations_df.merge(self.items_df, how = 'left', \n                                                          left_on = 'contentId', \n                                                          right_on = 'contentId')[['eventStrength', 'contentId', 'title', 'url', 'lang']]\n\n\n        return recommendations_df\n    \npopularity_model = PopularityRecommender(item_popularity_df, articles_df)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:48.810490Z","iopub.execute_input":"2022-05-05T14:01:48.810990Z","iopub.status.idle":"2022-05-05T14:01:48.822519Z","shell.execute_reply.started":"2022-05-05T14:01:48.810945Z","shell.execute_reply":"2022-05-05T14:01:48.820780Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print('Evaluating Popularity recommendation model...')\npop_global_metrics, pop_detailed_results_df = model_evaluator.evaluate_model(popularity_model)\nprint('\\nGlobal metrics:\\n%s' % pop_global_metrics)\npop_detailed_results_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:01:48.824831Z","iopub.execute_input":"2022-05-05T14:01:48.825410Z","iopub.status.idle":"2022-05-05T14:02:05.331557Z","shell.execute_reply.started":"2022-05-05T14:01:48.825360Z","shell.execute_reply":"2022-05-05T14:02:05.330711Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Here we perform the evaluation of the Popularity model, according to the method described above.\nIt achieved the Recall@5 of 0.2417, which means that about 24% of interacted items in test set were ranked by Popularity model among the top-5 items (from lists with 100 random items). And Recall@10 was even higher (37%), as expected.\nIt might be surprising to you that usually Popularity models could perform so well!","metadata":{}},{"cell_type":"markdown","source":"## Create a Content-Based Filtering Model\n\nWe recommend items similar to other items the user has interacted with in the past. This approach helps to avoid the cold-start problem. \n\nWe use the TF-IDF to convert unstructured text into a vectors. We can then compute the similarity between articles by using our vectors. ","metadata":{}},{"cell_type":"code","source":"#Ignoring stopwords (words with no semantics) from English and Portuguese (as we have a corpus with mixed languages)\nstopwords_list = stopwords.words('english') + stopwords.words('portuguese')\n\n#Trains a model whose vectors size is 5000, composed by the main unigrams and bigrams found in the corpus, ignoring stopwords\nvectorizer = TfidfVectorizer(analyzer='word',\n                     ngram_range=(1, 2),\n                     min_df=0.003,\n                     max_df=0.5,\n                     max_features=5000,\n                     stop_words=stopwords_list)\n\nitem_ids = articles_df['contentId'].tolist()\ntfidf_matrix = vectorizer.fit_transform(articles_df['title'] + \"\" + articles_df['text'])\ntfidf_feature_names = vectorizer.get_feature_names_out()\ntfidf_matrix","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:02:05.332875Z","iopub.execute_input":"2022-05-05T14:02:05.333095Z","iopub.status.idle":"2022-05-05T14:02:16.907477Z","shell.execute_reply.started":"2022-05-05T14:02:05.333068Z","shell.execute_reply":"2022-05-05T14:02:16.906578Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def get_item_profile(item_id):\n    idx = item_ids.index(item_id)\n    item_profile = tfidf_matrix[idx:idx+1]\n    return item_profile\n\ndef get_item_profiles(ids):\n    item_profiles_list = [get_item_profile(x) for x in ids]\n    item_profiles = scipy.sparse.vstack(item_profiles_list)\n    return item_profiles\n\ndef build_users_profile(person_id, interactions_indexed_df):\n    interactions_person_df = interactions_indexed_df.loc[person_id]\n    user_item_profiles = get_item_profiles(interactions_person_df['contentId'])\n    \n    user_item_strengths = np.array(interactions_person_df['eventStrength']).reshape(-1,1)\n    #Weighted average of item profiles by the interactions strength\n    user_item_strengths_weighted_avg = np.sum(user_item_profiles.multiply(user_item_strengths), axis=0) / np.sum(user_item_strengths)\n    user_profile_norm = sklearn.preprocessing.normalize(user_item_strengths_weighted_avg)\n    return user_profile_norm\n\ndef build_users_profiles(): \n    interactions_indexed_df = interactions_train_df[interactions_train_df['contentId'] \\\n                                                   .isin(articles_df['contentId'])].set_index('personId')\n    user_profiles = {}\n    for person_id in interactions_indexed_df.index.unique():\n        user_profiles[person_id] = build_users_profile(person_id, interactions_indexed_df)\n    return user_profiles","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:02:16.908693Z","iopub.execute_input":"2022-05-05T14:02:16.909104Z","iopub.status.idle":"2022-05-05T14:02:16.919691Z","shell.execute_reply.started":"2022-05-05T14:02:16.909069Z","shell.execute_reply":"2022-05-05T14:02:16.918865Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"user_profiles = build_users_profiles()\nlen(user_profiles)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:02:16.920577Z","iopub.execute_input":"2022-05-05T14:02:16.920834Z","iopub.status.idle":"2022-05-05T14:02:23.961685Z","shell.execute_reply.started":"2022-05-05T14:02:16.920807Z","shell.execute_reply":"2022-05-05T14:02:23.960827Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"myprofile = user_profiles[-1479311724257856983]\nprint(myprofile.shape)\npd.DataFrame(sorted(zip(tfidf_feature_names, \n                        user_profiles[-1479311724257856983].flatten().tolist()), key=lambda x: -x[1])[:20],\n             columns=['token', 'relevance'])","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:02:23.963203Z","iopub.execute_input":"2022-05-05T14:02:23.963505Z","iopub.status.idle":"2022-05-05T14:02:23.983984Z","shell.execute_reply.started":"2022-05-05T14:02:23.963464Z","shell.execute_reply":"2022-05-05T14:02:23.982574Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class ContentBasedRecommender:\n    \n    MODEL_NAME = 'Content-Based'\n    \n    def __init__(self, items_df=None):\n        self.item_ids = item_ids\n        self.items_df = items_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n        \n    def _get_similar_items_to_user_profile(self, person_id, topn=1000):\n        #Computes the cosine similarity between the user profile and all item profiles\n        cosine_similarities = cosine_similarity(user_profiles[person_id], tfidf_matrix)\n        #Gets the top similar items\n        similar_indices = cosine_similarities.argsort().flatten()[-topn:]\n        #Sort the similar items by similarity\n        similar_items = sorted([(item_ids[i], cosine_similarities[0,i]) for i in similar_indices], key=lambda x: -x[1])\n        return similar_items\n        \n    def recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):\n        similar_items = self._get_similar_items_to_user_profile(user_id)\n        #Ignores items the user has already interacted\n        similar_items_filtered = list(filter(lambda x: x[0] not in items_to_ignore, similar_items))\n        \n        recommendations_df = pd.DataFrame(similar_items_filtered, columns=['contentId', 'recStrength']) \\\n                                    .head(topn)\n\n        if verbose:\n            if self.items_df is None:\n                raise Exception('\"items_df\" is required in verbose mode')\n\n            recommendations_df = recommendations_df.merge(self.items_df, how = 'left', \n                                                          left_on = 'contentId', \n                                                          right_on = 'contentId')[['recStrength', 'contentId', 'title', 'url', 'lang']]\n\n\n        return recommendations_df\n    \ncontent_based_recommender_model = ContentBasedRecommender(articles_df)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:02:23.986324Z","iopub.execute_input":"2022-05-05T14:02:23.986673Z","iopub.status.idle":"2022-05-05T14:02:23.999728Z","shell.execute_reply.started":"2022-05-05T14:02:23.986617Z","shell.execute_reply":"2022-05-05T14:02:23.998974Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print('Evaluating Content-Based Filtering model...')\ncb_global_metrics, cb_detailed_results_df = model_evaluator.evaluate_model(content_based_recommender_model)\nprint('\\nGlobal metrics:\\n%s' % cb_global_metrics)\ncb_detailed_results_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:02:24.000740Z","iopub.execute_input":"2022-05-05T14:02:24.001355Z","iopub.status.idle":"2022-05-05T14:02:50.402561Z","shell.execute_reply.started":"2022-05-05T14:02:24.001323Z","shell.execute_reply":"2022-05-05T14:02:50.401582Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"With personalized recommendations of content-based filtering model, we have a Recall@5 to about 0.162, which means that about 16% of interacted items in test set were ranked by this model among the top-5 items (from lists with 100 random items). And Recall@10 was 0.261 (52%). The lower performance of the Content-Based model compared to the Popularity model may indicate that users are not that fixed in content very similar to their previous reads.","metadata":{}},{"cell_type":"markdown","source":"# Create Collaborative Filtering Model\n\n","metadata":{}},{"cell_type":"code","source":"#Creating a sparse pivot table with users in rows and items in columns\nusers_items_pivot_matrix_df = interactions_train_df.pivot(index='personId', \n                                                          columns='contentId', \n                                                          values='eventStrength').fillna(0)\n\nusers_items_pivot_matrix_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:02:50.403840Z","iopub.execute_input":"2022-05-05T14:02:50.404067Z","iopub.status.idle":"2022-05-05T14:02:50.511804Z","shell.execute_reply.started":"2022-05-05T14:02:50.404039Z","shell.execute_reply":"2022-05-05T14:02:50.510599Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"users_items_pivot_matrix = users_items_pivot_matrix_df.values\nusers_items_pivot_matrix[:10]","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:03:03.035327Z","iopub.execute_input":"2022-05-05T14:03:03.035592Z","iopub.status.idle":"2022-05-05T14:03:03.042507Z","shell.execute_reply.started":"2022-05-05T14:03:03.035563Z","shell.execute_reply":"2022-05-05T14:03:03.041737Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"users_ids = list(users_items_pivot_matrix_df.index)\nusers_ids[:10]","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:03:07.996669Z","iopub.execute_input":"2022-05-05T14:03:07.997501Z","iopub.status.idle":"2022-05-05T14:03:08.003744Z","shell.execute_reply.started":"2022-05-05T14:03:07.997446Z","shell.execute_reply":"2022-05-05T14:03:08.003080Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"users_items_pivot_sparse_matrix = csr_matrix(users_items_pivot_matrix)\nusers_items_pivot_sparse_matrix","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:03:21.204218Z","iopub.execute_input":"2022-05-05T14:03:21.204674Z","iopub.status.idle":"2022-05-05T14:03:21.252902Z","shell.execute_reply.started":"2022-05-05T14:03:21.204629Z","shell.execute_reply":"2022-05-05T14:03:21.251961Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#The number of factors to factor the user-item matrix.\nNUMBER_OF_FACTORS_MF = 15\n#Performs matrix factorization of the original user item matrix\n#U, sigma, Vt = svds(users_items_pivot_matrix, k = NUMBER_OF_FACTORS_MF)\nU, sigma, Vt = svds(users_items_pivot_sparse_matrix, k = NUMBER_OF_FACTORS_MF)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:03:34.550768Z","iopub.execute_input":"2022-05-05T14:03:34.551039Z","iopub.status.idle":"2022-05-05T14:03:34.601769Z","shell.execute_reply.started":"2022-05-05T14:03:34.551011Z","shell.execute_reply":"2022-05-05T14:03:34.600729Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"U.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:03:59.844581Z","iopub.execute_input":"2022-05-05T14:03:59.844858Z","iopub.status.idle":"2022-05-05T14:03:59.850846Z","shell.execute_reply.started":"2022-05-05T14:03:59.844829Z","shell.execute_reply":"2022-05-05T14:03:59.850134Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"Vt.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:04:06.923925Z","iopub.execute_input":"2022-05-05T14:04:06.924438Z","iopub.status.idle":"2022-05-05T14:04:06.930465Z","shell.execute_reply.started":"2022-05-05T14:04:06.924403Z","shell.execute_reply":"2022-05-05T14:04:06.929448Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"sigma = np.diag(sigma)\nsigma.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:04:13.472043Z","iopub.execute_input":"2022-05-05T14:04:13.472626Z","iopub.status.idle":"2022-05-05T14:04:13.479531Z","shell.execute_reply.started":"2022-05-05T14:04:13.472576Z","shell.execute_reply":"2022-05-05T14:04:13.478742Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) \nall_user_predicted_ratings","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:04:24.340690Z","iopub.execute_input":"2022-05-05T14:04:24.341010Z","iopub.status.idle":"2022-05-05T14:04:24.364453Z","shell.execute_reply.started":"2022-05-05T14:04:24.340979Z","shell.execute_reply":"2022-05-05T14:04:24.363921Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"all_user_predicted_ratings_norm = (all_user_predicted_ratings - all_user_predicted_ratings.min()) / (all_user_predicted_ratings.max() - all_user_predicted_ratings.min())","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:04:59.573740Z","iopub.execute_input":"2022-05-05T14:04:59.573995Z","iopub.status.idle":"2022-05-05T14:04:59.598620Z","shell.execute_reply.started":"2022-05-05T14:04:59.573968Z","shell.execute_reply":"2022-05-05T14:04:59.597732Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"#Converting the reconstructed matrix back to a Pandas dataframe\ncf_preds_df = pd.DataFrame(all_user_predicted_ratings_norm, columns = users_items_pivot_matrix_df.columns, index=users_ids).transpose()\ncf_preds_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:05:11.994705Z","iopub.execute_input":"2022-05-05T14:05:11.995006Z","iopub.status.idle":"2022-05-05T14:05:12.030533Z","shell.execute_reply.started":"2022-05-05T14:05:11.994967Z","shell.execute_reply":"2022-05-05T14:05:12.030004Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"len(cf_preds_df.columns)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:05:38.949707Z","iopub.execute_input":"2022-05-05T14:05:38.950129Z","iopub.status.idle":"2022-05-05T14:05:38.956753Z","shell.execute_reply.started":"2022-05-05T14:05:38.950098Z","shell.execute_reply":"2022-05-05T14:05:38.955953Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"class CFRecommender:\n    \n    MODEL_NAME = 'Collaborative Filtering'\n    \n    def __init__(self, cf_predictions_df, items_df=None):\n        self.cf_predictions_df = cf_predictions_df\n        self.items_df = items_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n        \n    def recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):\n        # Get and sort the user's predictions\n        sorted_user_predictions = self.cf_predictions_df[user_id].sort_values(ascending=False) \\\n                                    .reset_index().rename(columns={user_id: 'recStrength'})\n\n        # Recommend the highest predicted rating movies that the user hasn't seen yet.\n        recommendations_df = sorted_user_predictions[~sorted_user_predictions['contentId'].isin(items_to_ignore)] \\\n                               .sort_values('recStrength', ascending = False) \\\n                               .head(topn)\n\n        if verbose:\n            if self.items_df is None:\n                raise Exception('\"items_df\" is required in verbose mode')\n\n            recommendations_df = recommendations_df.merge(self.items_df, how = 'left', \n                                                          left_on = 'contentId', \n                                                          right_on = 'contentId')[['recStrength', 'contentId', 'title', 'url', 'lang']]\n\n\n        return recommendations_df\n    \ncf_recommender_model = CFRecommender(cf_preds_df, articles_df)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:05:51.326274Z","iopub.execute_input":"2022-05-05T14:05:51.327194Z","iopub.status.idle":"2022-05-05T14:05:51.337945Z","shell.execute_reply.started":"2022-05-05T14:05:51.327152Z","shell.execute_reply":"2022-05-05T14:05:51.336900Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print('Evaluating Collaborative Filtering (SVD Matrix Factorization) model...')\ncf_global_metrics, cf_detailed_results_df = model_evaluator.evaluate_model(cf_recommender_model)\nprint('\\nGlobal metrics:\\n%s' % cf_global_metrics)\ncf_detailed_results_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:06:07.523978Z","iopub.execute_input":"2022-05-05T14:06:07.524651Z","iopub.status.idle":"2022-05-05T14:06:26.002136Z","shell.execute_reply.started":"2022-05-05T14:06:07.524598Z","shell.execute_reply":"2022-05-05T14:06:26.001296Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Evaluating the Collaborative Filtering model (SVD matrix factorization), we observe that we got Recall@5 (33%) and Recall@10 (46%) values, much higher than Popularity model and Content-Based model.","metadata":{}},{"cell_type":"markdown","source":"# Create Hybrid Recommender\n\nCombine Collaborative Filtering and Content-Based Filtering.","metadata":{}},{"cell_type":"code","source":"class HybridRecommender:\n    \n    MODEL_NAME = 'Hybrid'\n    \n    def __init__(self, cb_rec_model, cf_rec_model, items_df, cb_ensemble_weight=1.0, cf_ensemble_weight=1.0):\n        self.cb_rec_model = cb_rec_model\n        self.cf_rec_model = cf_rec_model\n        self.cb_ensemble_weight = cb_ensemble_weight\n        self.cf_ensemble_weight = cf_ensemble_weight\n        self.items_df = items_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n        \n    def recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):\n        #Getting the top-1000 Content-based filtering recommendations\n        cb_recs_df = self.cb_rec_model.recommend_items(user_id, items_to_ignore=items_to_ignore, verbose=verbose,\n                                                           topn=1000).rename(columns={'recStrength': 'recStrengthCB'})\n        \n        #Getting the top-1000 Collaborative filtering recommendations\n        cf_recs_df = self.cf_rec_model.recommend_items(user_id, items_to_ignore=items_to_ignore, verbose=verbose, \n                                                           topn=1000).rename(columns={'recStrength': 'recStrengthCF'})\n        \n        #Combining the results by contentId\n        recs_df = cb_recs_df.merge(cf_recs_df,\n                                   how = 'outer', \n                                   left_on = 'contentId', \n                                   right_on = 'contentId').fillna(0.0)\n        \n        #Computing a hybrid recommendation score based on CF and CB scores\n        #recs_df['recStrengthHybrid'] = recs_df['recStrengthCB'] * recs_df['recStrengthCF'] \n        recs_df['recStrengthHybrid'] = (recs_df['recStrengthCB'] * self.cb_ensemble_weight) \\\n                                     + (recs_df['recStrengthCF'] * self.cf_ensemble_weight)\n        \n        #Sorting recommendations by hybrid score\n        recommendations_df = recs_df.sort_values('recStrengthHybrid', ascending=False).head(topn)\n\n        if verbose:\n            if self.items_df is None:\n                raise Exception('\"items_df\" is required in verbose mode')\n\n            recommendations_df = recommendations_df.merge(self.items_df, how = 'left', \n                                                          left_on = 'contentId', \n                                                          right_on = 'contentId')[['recStrengthHybrid', 'contentId', 'title', 'url', 'lang']]\n\n\n        return recommendations_df\n    \nhybrid_recommender_model = HybridRecommender(content_based_recommender_model, cf_recommender_model, articles_df,\n                                             cb_ensemble_weight=1.0, cf_ensemble_weight=100.0)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:07:59.444344Z","iopub.execute_input":"2022-05-05T14:07:59.444633Z","iopub.status.idle":"2022-05-05T14:07:59.458315Z","shell.execute_reply.started":"2022-05-05T14:07:59.444589Z","shell.execute_reply":"2022-05-05T14:07:59.457278Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"print('Evaluating Hybrid model...')\nhybrid_global_metrics, hybrid_detailed_results_df = model_evaluator.evaluate_model(hybrid_recommender_model)\nprint('\\nGlobal metrics:\\n%s' % hybrid_global_metrics)\nhybrid_detailed_results_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:08:08.994013Z","iopub.execute_input":"2022-05-05T14:08:08.994298Z","iopub.status.idle":"2022-05-05T14:08:45.134593Z","shell.execute_reply.started":"2022-05-05T14:08:08.994269Z","shell.execute_reply":"2022-05-05T14:08:45.133750Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"We have a new champion!\nOur simple hybrid approach surpasses Content-Based filtering with its combination with Collaborative Filtering. Now we have a Recall@5 of 34.2% and Recall@10 of 47.9%","metadata":{}},{"cell_type":"markdown","source":"# Lets compare the methods","metadata":{}},{"cell_type":"code","source":"global_metrics_df = pd.DataFrame([cb_global_metrics, pop_global_metrics, cf_global_metrics, hybrid_global_metrics]) \\\n                        .set_index('modelName')\nglobal_metrics_df","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:09:01.230999Z","iopub.execute_input":"2022-05-05T14:09:01.231285Z","iopub.status.idle":"2022-05-05T14:09:01.243823Z","shell.execute_reply.started":"2022-05-05T14:09:01.231253Z","shell.execute_reply":"2022-05-05T14:09:01.242760Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nax = global_metrics_df.transpose().plot(kind='bar', figsize=(15,8))\nfor p in ax.patches:\n    ax.annotate(\"%.3f\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:09:13.643740Z","iopub.execute_input":"2022-05-05T14:09:13.644309Z","iopub.status.idle":"2022-05-05T14:09:13.978556Z","shell.execute_reply.started":"2022-05-05T14:09:13.644275Z","shell.execute_reply":"2022-05-05T14:09:13.977639Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Lets test a single user's recommendations","metadata":{}},{"cell_type":"code","source":"def inspect_interactions(person_id, test_set=True):\n    if test_set:\n        interactions_df = interactions_test_indexed_df\n    else:\n        interactions_df = interactions_train_indexed_df\n    return interactions_df.loc[person_id].merge(articles_df, how = 'left', \n                                                      left_on = 'contentId', \n                                                      right_on = 'contentId') \\\n                          .sort_values('eventStrength', ascending = False)[['eventStrength', \n                                                                          'contentId',\n                                                                          'title', 'url', 'lang']]","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:11:48.276858Z","iopub.execute_input":"2022-05-05T14:11:48.277180Z","iopub.status.idle":"2022-05-05T14:11:48.284307Z","shell.execute_reply.started":"2022-05-05T14:11:48.277149Z","shell.execute_reply":"2022-05-05T14:11:48.283193Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"inspect_interactions(-1479311724257856983, test_set=False).head(20)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:11:57.162729Z","iopub.execute_input":"2022-05-05T14:11:57.163007Z","iopub.status.idle":"2022-05-05T14:11:57.187478Z","shell.execute_reply.started":"2022-05-05T14:11:57.162976Z","shell.execute_reply":"2022-05-05T14:11:57.186708Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"hybrid_recommender_model.recommend_items(-1479311724257856983, topn=20, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:12:12.605889Z","iopub.execute_input":"2022-05-05T14:12:12.606353Z","iopub.status.idle":"2022-05-05T14:12:12.666631Z","shell.execute_reply.started":"2022-05-05T14:12:12.606301Z","shell.execute_reply":"2022-05-05T14:12:12.665367Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nColaborative Filtering does much better than both Popularity and Content Based Filtering. \n\nOur best scores come from a hybrid approach. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}